---
title: "Random Hyperparameter Search"
author: Ignat Drozdov
date: November 17, 2017
output:
  md_document:
        variant: markdown_github
---

# Background
Random search is a simple and effective technique for hyperparameter optimization. Random search has a probability of 95% of finding a combination of parameters within the 5% optima with only 60 iterations (see [Bergstra and Bengio](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)).

# Installation

The latest version can be installed through:

```{r, eval=FALSE}
devtools::install_github("beringresearch/ABC/RandomParameterSearch")
```

# Example

Let's perform a random search to identify a comination of trees/mtry that maximise overall accuracy of a Random Forest algorithm on the Iris dataset.

NOTE: This is a poor example because the there is a strong separability between Iris classes, thus classification is not sensitive to hyperparameter variability.

First we create training and validation sets:

```{r}
library(randomForest)
library(RandomParameterSearch)

set.seed(1234)
frac <- round(0.75 * nrow(iris))
trainIndex <- sample(1:nrow(iris), frac, replace = FALSE)

x_train <- iris[trainIndex, 1:4]
y_train <- iris[trainIndex, 5]
x_test <- iris[-trainIndex, 1:4]
y_test <- iris[-trainIndex, 5]
```

Now let's set up our search space:

```{r}
params <- list(ntree = c(1L, 500L),
               mtry = c(1L, 4L))
grid <- create_random_grid(nrounds = 100, params = params, seed = 1234)
grid
```

And a function that we wish to evaluate:

```{r}
func <- function(ntree, mtry){
  rf <- randomForest(x = x_train, y = y_train, ntree = ntree, mtry = mtry)
  yh <- predict(rf, x_test, type = "prob")
  lbl <- colnames(yh)[apply(yh, 1, which.max)]

  sum(lbl == y_test)/length(y_test)
}
```

Finally, setting up the random search:

```{r}
search <- random_search(func, grid = grid)

head(search)
```

```{r}
library(ggplot2)
library(GGally)

ggpairs(search)
```


